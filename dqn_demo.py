#!/usr/bin/env python3
"""
DQN (Deep Q-Network) ä½¿ç”¨ç¤ºä¾‹è„šæœ¬

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•åœ¨äº¤é€šè¿è¾“ä¼˜åŒ–ç³»ç»Ÿä¸­ä½¿ç”¨ DQN è¿›è¡Œå®æ—¶å†³ç­–ä¼˜åŒ–ã€‚
"""

import os
import sys
import django
from datetime import timedelta

# è®¾ç½® Django ç¯å¢ƒ
sys.path.append('.')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'transportation_optimization_backend.settings')
django.setup()

from train_optimization.dqn_optimization import DQNOptimizer, DQNAgent, TransportationEnvironment
from train_optimization.models import Train, Route, OptimizationTask
from django.contrib.auth.models import User


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    print("ğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    
    # åˆ›å»ºåˆ—è½¦æ•°æ®
    trains = [
        {
            'train_id': 'EXPRESS_001',
            'train_type': 'express', 
            'capacity': 300,
            'max_speed': 160.0,
            'fuel_efficiency': 15.2,
            'maintenance_cost_per_km': 3.50
        },
        {
            'train_id': 'LOCAL_002',
            'train_type': 'local',
            'capacity': 150,
            'max_speed': 80.0, 
            'fuel_efficiency': 18.5,
            'maintenance_cost_per_km': 2.20
        },
        {
            'train_id': 'HIGH_SPEED_003',
            'train_type': 'high_speed',
            'capacity': 400,
            'max_speed': 200.0,
            'fuel_efficiency': 12.8,
            'maintenance_cost_per_km': 5.00
        }
    ]
    
    created_trains = []
    for train_data in trains:
        train, created = Train.objects.get_or_create(
            train_id=train_data['train_id'],
            defaults={**train_data, 'is_operational': True}
        )
        created_trains.append(train)
        print(f"   âœ… åˆ—è½¦ {train.train_id} ({'åˆ›å»º' if created else 'å·²å­˜åœ¨'})")
    
    # åˆ›å»ºè·¯çº¿æ•°æ®
    routes = [
        {
            'name': 'ä¸»å¹²çº¿',
            'start_station': 'ä¸­å¤®è½¦ç«™',
            'end_station': 'æœºåœºç«™',
            'distance': 45.0,
            'estimated_travel_time': timedelta(hours=1, minutes=10)
        },
        {
            'name': 'å¸‚åŒºç¯çº¿',
            'start_station': 'å¸‚ä¸­å¿ƒ',
            'end_station': 'å•†ä¸šåŒº',
            'distance': 25.0,
            'estimated_travel_time': timedelta(minutes=35)
        },
        {
            'name': 'é«˜é€Ÿä¸“çº¿',
            'start_station': 'åŒ—ç«™',
            'end_station': 'å—ç«™',
            'distance': 120.0,
            'estimated_travel_time': timedelta(hours=1, minutes=30)
        }
    ]
    
    created_routes = []
    for route_data in routes:
        route, created = Route.objects.get_or_create(
            name=route_data['name'],
            defaults={**route_data, 'is_active': True}
        )
        created_routes.append(route)
        print(f"   âœ… è·¯çº¿ {route.name} ({'åˆ›å»º' if created else 'å·²å­˜åœ¨'})")
    
    return created_trains, created_routes


def demonstrate_dqn_environment():
    """æ¼”ç¤º DQN ç¯å¢ƒåŠŸèƒ½"""
    print("\nğŸ¤– DQN ç¯å¢ƒæ¼”ç¤º")
    
    trains = Train.objects.filter(is_operational=True)[:3]
    routes = Route.objects.filter(is_active=True)[:3]
    
    # åˆ›å»ºç¯å¢ƒ
    env = TransportationEnvironment(list(trains), list(routes))
    print(f"   çŠ¶æ€ç©ºé—´ç»´åº¦: {env.state_size}")
    print(f"   åŠ¨ä½œç©ºé—´å¤§å°: {env.action_size}")
    
    # é‡ç½®ç¯å¢ƒ
    state = env.reset()
    print(f"   åˆå§‹çŠ¶æ€: {[f'{x:.3f}' for x in state[:6]]}...")
    
    # æ¼”ç¤ºå‡ ä¸ªåŠ¨ä½œ
    actions = ['è°ƒåº¦åˆ—è½¦', 'å»¶è¯¯å‘è½¦', 'æ”¹å˜è·¯çº¿', 'ä¼˜åŒ–é€Ÿåº¦']
    
    for i, action_name in enumerate(actions):
        next_state, reward, done, info = env.step(i)
        print(f"   æ‰§è¡Œ '{action_name}': å¥–åŠ±={reward:.3f}, å®Œæˆ={done}")
        
    return env


def demonstrate_dqn_agent():
    """æ¼”ç¤º DQN æ™ºèƒ½ä½“åŠŸèƒ½"""
    print("\nğŸ§  DQN æ™ºèƒ½ä½“æ¼”ç¤º")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = DQNAgent(state_size=12, action_size=4, learning_rate=0.001)
    print(f"   æ™ºèƒ½ä½“å‚æ•°:")
    print(f"     å­¦ä¹ ç‡: {agent.learning_rate}")
    print(f"     æ¢ç´¢ç‡: {agent.epsilon}")
    print(f"     æ‰¹æ¬¡å¤§å°: {agent.batch_size}")
    
    # æµ‹è¯•å†³ç­–
    test_state = [0.5, 0.8, 0.3, 5.0, 0.5, 0.4, 0.7, 0.8, 0.6, 0.4, 0.3, 0.5]
    action = agent.act(test_state)
    action_names = {0: "è°ƒåº¦åˆ—è½¦", 1: "å»¶è¯¯å‘è½¦", 2: "æ”¹å˜è·¯çº¿", 3: "ä¼˜åŒ–é€Ÿåº¦"}
    print(f"   æµ‹è¯•çŠ¶æ€ä¸‹é€‰æ‹©çš„åŠ¨ä½œ: {action_names[action]}")
    
    return agent


def demonstrate_dqn_optimization():
    """æ¼”ç¤ºå®Œæ•´çš„ DQN ä¼˜åŒ–è¿‡ç¨‹"""
    print("\nğŸ¯ DQN ä¼˜åŒ–æ¼”ç¤º")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = DQNOptimizer()
    
    # è®¾ç½®è®­ç»ƒå‚æ•°
    parameters = {
        'episodes': 10,          # è®­ç»ƒè½®æ•°
        'max_steps': 50,         # æ¯è½®æœ€å¤§æ­¥æ•°
        'learning_rate': 0.001   # å­¦ä¹ ç‡
    }
    
    print("   å¼€å§‹ DQN è®­ç»ƒä¼˜åŒ–...")
    print(f"   å‚æ•°: {parameters}")
    
    # æ‰§è¡Œä¼˜åŒ–
    results = optimizer.optimize(parameters)
    
    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“ˆ ä¼˜åŒ–ç»“æœ:")
    print(f"   ç®—æ³•: {results.get('algorithm', 'Unknown')}")
    print(f"   è®­ç»ƒè½®æ•°: {results.get('episodes_trained', 0)}")
    print(f"   å¹³å‡å¥–åŠ±: {results.get('average_reward', 0):.3f}")
    print(f"   æœ€ç»ˆæ¢ç´¢ç‡: {results.get('final_epsilon', 0):.3f}")
    print(f"   æ¨¡å‹å·²ä¿å­˜: {results.get('model_saved', False)}")
    
    print("\nğŸ¯ æ€§èƒ½æ”¹è¿›é¢„æµ‹:")
    improvements = results.get('performance_improvements', {})
    for metric, value in improvements.items():
        metric_names = {
            'fuel_efficiency_gain': 'ç‡ƒæ²¹æ•ˆç‡æå‡',
            'schedule_optimization': 'è°ƒåº¦ä¼˜åŒ–æ”¹è¿›', 
            'route_utilization': 'è·¯çº¿åˆ©ç”¨ç‡æé«˜',
            'delay_reduction': 'å»¶è¯¯å‡å°‘'
        }
        print(f"   {metric_names.get(metric, metric)}: {value}")
    
    print("\nğŸ’¡ ç³»ç»Ÿå»ºè®®:")
    recommendations = results.get('recommendations', [])
    for i, rec in enumerate(recommendations[:5], 1):
        print(f"   {i}. {rec}")
    
    return results


def demonstrate_real_time_prediction():
    """æ¼”ç¤ºå®æ—¶å†³ç­–é¢„æµ‹"""
    print("\nâš¡ å®æ—¶å†³ç­–é¢„æµ‹æ¼”ç¤º")
    
    optimizer = DQNOptimizer()
    
    # æ¨¡æ‹Ÿå½“å‰ç³»ç»ŸçŠ¶æ€
    current_states = [
        {
            "scenario": "é«˜å³°æ—¶æ®µ",
            "state": {
                "busy_ratio": 0.8,      # 80% åˆ—è½¦åœ¨è¿è¡Œ
                "avg_fuel": 0.6,        # 60% ç‡ƒæ²¹æ°´å¹³
                "congestion_ratio": 0.7, # 70% è·¯çº¿æ‹¥å µ
                "avg_delay": 8.0,       # å¹³å‡å»¶è¯¯ 8 åˆ†é’Ÿ
                "fuel_efficiency": 0.65,
                "delay_penalty": 0.7,
                "passenger_load": 0.9,
                "fleet_size": 25,
                "network_size": 15
            }
        },
        {
            "scenario": "ä½å³°æ—¶æ®µ", 
            "state": {
                "busy_ratio": 0.3,      # 30% åˆ—è½¦åœ¨è¿è¡Œ
                "avg_fuel": 0.9,        # 90% ç‡ƒæ²¹æ°´å¹³
                "congestion_ratio": 0.1, # 10% è·¯çº¿æ‹¥å µ
                "avg_delay": 2.0,       # å¹³å‡å»¶è¯¯ 2 åˆ†é’Ÿ
                "fuel_efficiency": 0.85,
                "delay_penalty": 0.95,
                "passenger_load": 0.4,
                "fleet_size": 25,
                "network_size": 15
            }
        }
    ]
    
    # ä¸ºæ¯ä¸ªåœºæ™¯é¢„æµ‹æœ€ä¼˜åŠ¨ä½œ
    for scenario_data in current_states:
        scenario = scenario_data["scenario"]
        state = scenario_data["state"]
        
        print(f"\n   åœºæ™¯: {scenario}")
        print(f"   å½“å‰çŠ¶æ€: å¿™ç¢Œç‡={state['busy_ratio']:.1%}, "
              f"æ‹¥å µç‡={state['congestion_ratio']:.1%}, "
              f"å¹³å‡å»¶è¯¯={state['avg_delay']:.1f}åˆ†é’Ÿ")
        
        # éœ€è¦å…ˆåˆå§‹åŒ–æ™ºèƒ½ä½“
        optimizer.agent = DQNAgent(state_size=12, action_size=4)
        
        prediction = optimizer.predict_optimal_action(state)
        
        action_descriptions = {
            "schedule_train": "è°ƒåº¦æ–°åˆ—è½¦ - å¢åŠ è¿åŠ›åº”å¯¹éœ€æ±‚",
            "delay_departure": "å»¶è¯¯å‘è½¦ - ä¼˜åŒ–æ—¶åˆ»è¡¨é—´éš”",
            "change_route": "æ”¹å˜è·¯çº¿ - é¿å¼€æ‹¥å µè·¯æ®µ",
            "optimize_speed": "ä¼˜åŒ–é€Ÿåº¦ - æé«˜ç‡ƒæ²¹æ•ˆç‡"
        }
        
        optimal_action = prediction.get('optimal_action', 'unknown')
        confidence = prediction.get('confidence', 0)
        
        print(f"   ğŸ¯ æ¨èåŠ¨ä½œ: {action_descriptions.get(optimal_action, optimal_action)}")
        print(f"   ğŸ“Š ç½®ä¿¡åº¦: {confidence:.1%}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš„ DQN äº¤é€šè¿è¾“ä¼˜åŒ–ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    try:
        # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®
        trains, routes = create_sample_data()
        
        # 2. æ¼”ç¤º DQN ç¯å¢ƒ
        env = demonstrate_dqn_environment()
        
        # 3. æ¼”ç¤º DQN æ™ºèƒ½ä½“
        agent = demonstrate_dqn_agent()
        
        # 4. æ¼”ç¤ºå®Œæ•´ä¼˜åŒ–
        results = demonstrate_dqn_optimization()
        
        # 5. æ¼”ç¤ºå®æ—¶å†³ç­–
        demonstrate_real_time_prediction()
        
        print("\n" + "=" * 50)
        print("âœ… DQN ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“‹ æ€»ç»“:")
        print("   - DQN èƒ½å¤Ÿå¤„ç†å¤æ‚çš„å®æ—¶äº¤é€šè°ƒåº¦é—®é¢˜")
        print("   - é€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨ä¼˜åŒ–å¤šä¸ªç›®æ ‡")
        print("   - é€‚åº”åŠ¨æ€å˜åŒ–çš„äº¤é€šç¯å¢ƒ")
        print("   - æä¾›å®æ—¶å†³ç­–æ”¯æŒ")
        print("\nğŸ”— æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ DQN_USAGE_ANALYSIS.md")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()